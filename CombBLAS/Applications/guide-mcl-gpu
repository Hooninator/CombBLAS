* Obtaining repository *
  Get it from
  https://bitbucket.org/berkeleylab/combinatorial-blas-2.0/src/master/
  and checkout to branch hipmcl-gpu

* Compilation on Cori GPU nodes *
module commands prior to compilation:
  module load esslurm
  module load cuda/9.2
  module swap gcc/7.3.0 gcc/6.1.0 [boost conflicts with gcc/7.3.0]
  module load boost
  module swap cray-mpich openmpi
compile by running (under CombBLAS/Applications directory):
  make -f Makefile_mcl_gpu-cori

* Compilation on Summit *
module commands prior to compilation:
  module load cuda/9.2.148
  module load gcc/5.4.0
  module load boost
compile by running (under CombBLAS/Applications directory):
  make -f Makefile_mcl_gpu-summit

* Running *
The GPU version of HipMCL uses both the CPU and GPU for certain tasks. The CPU
section is multi-threaded (so it is useful to allocate a number of cores and use
export OMP_NUM_THREADS to set the number of threads for CPU to use not to make
CPU a bottleneck) and the GPU section can handle multiple GPUs on a node. The
tasks run on GPU and CPU are interleaved in several aspects.
Example run command:
  cori: mpirun -n 1 ./mcl_gpu -M Renamed_vir_vs_vir_30_50length.indexed.mtx --matrix-market -base 0 -I 2 --32bit-local-index -per-process-mem 64 -lspgemm hybrid --nrounds 7
  summit (single GPU): jsrun -n 1 -r 1 -a 1 -g 1 -c 42 -bpacked:42 ./mcl_gpu -M Renamed_vir_vs_vir_30_50length.indexed.mtx --matrix-market -base 0 -I 2 --32bit-local-index -per-process-mem 96 -lspgemm nsparse --nrounds 7
  summit (multiple GPUs): jsrun -n 1 -r 1 -a 1 -g 6 -c 42 -bpacked:42 ./mcl_gpu -M Renamed_vir_vs_vir_30_50length.indexed.mtx --matrix-market -base 0 -I 2 --32bit-local-index -per-process-mem 96 -lspgemm nsparse --nrounds 7

* Options *
In addition to already existing options in HipMCL, there are two additional
options:
lspgemm: how to perform local SpGEMM (sparse matrix-sparse matrix
  multiplication). It can be one of "cpu hybrid rmerge2 nsparse bhsparse". If
  you choose "cpu" option everything will be run on the CPU. The options
  "rmerge2", "nsparse", and "bhsparse" use different SpGEMM variants for
  performing SpGEMM on GPU. "nsparse" is often the fastest. The "hybrid" option
  selects one of these (including cpu) according to some criteria. I generally
  use the "hybrid" option.
nrounds: This is the number of samples used for memory estimation. A value
  between 5 and 10 is expected to perform well.
It is critical that you use "--32bit-local-indices" option for running the code
as the GPU codes currently do not support 64 bit versions.
In addition, it is recommended that if you run into memory problems, you
decrease the value used for -per-process-mem

* Codes *
The files related to GPU implementation are under CombBLAS/gspgemm. The main
file of MCL code is CombBLAS/Applications/MCL.cpp. There exists a symbolic link
MCL.cu to this file to compile with nvcc.
